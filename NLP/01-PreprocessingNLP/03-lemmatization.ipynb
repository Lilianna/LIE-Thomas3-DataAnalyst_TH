{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "As we saw in the previous chapter, we can explain to the machine which words are similar but also how different there are.\n",
    "\n",
    "However some \"different\" words are only variations of the same word and should not be considered as different entries. \n",
    "\n",
    "Let's take an example:\n",
    "\n",
    "Imagine that you are asked to build a model to classify books in two categories: _cooking_ and _cars_. You will use the most frequent words of the book to build your algorithm.\n",
    "\n",
    "In that case you don't really want to make a distinction between `apple` and `apples` or between `wheel` and `wheels`. You prefer to consider `apple` and `apples` as being variations of `apple`.\n",
    "\n",
    "To fix that, we will apply **lemmatization**. This approach aims to reduce each word to its simplest variation (named **lemma**). This lemma corresponds to the heading word in a language dictionary:\n",
    "\n",
    "\n",
    "**apple** (noun) : `a round fruit (usually with a green or red skin) which can be eaten (plural: apples)`\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Still confused?\n",
    "Let's see how it works in a practical case.\n",
    "\n",
    "First, read [this article](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/).\n",
    "\n",
    "Then, try to apply what you have learned by using SpaCy or NLTK.\n",
    "\n",
    "**Pro tips:** Most lemmatizers only work with a single word and not on sentences. Think about tokenizing your sentence first.\n",
    "\n",
    "**Pro tips:** If you experience SSL issues during `nltk` import [check this](https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you lemmatize this sentence with Spacy and / or NLTK?\n",
    "\n",
    "my_sentence = \"Those children are playing. this game, those games, I play he plays\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\becode\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\becode\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "Those children are playing. this game, those games, I play he plays\n",
      "\n",
      "After Lowercasing:\n",
      "those children are playing. this game, those games, i play he plays\n",
      "\n",
      "After Removing Punctuation:\n",
      "those children are playing this game those games i play he plays\n",
      "\n",
      "After Handling Contractions:\n",
      "those children are playing this game those games i play he plays\n",
      "\n",
      "After Tokenization and Lemmatization (NLTK):\n",
      "those child are playing this game those game i play he play\n",
      "\n",
      "After Tokenization and Lemmatization (SpaCy):\n",
      "those child be play this game those game I play he play\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import contractions\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Example sentence\n",
    "my_sentence = \"Those children are playing. this game, those games, I play he plays\"\n",
    "print(\"Original Sentence:\")\n",
    "print(my_sentence)\n",
    "\n",
    "# Text normalization using lowercasing\n",
    "normalized_sentence = my_sentence.lower()\n",
    "print(\"\\nAfter Lowercasing:\")\n",
    "print(normalized_sentence)\n",
    "\n",
    "# Text normalization: removing punctuation\n",
    "normalized_sentence = normalized_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(\"\\nAfter Removing Punctuation:\")\n",
    "print(normalized_sentence)\n",
    "\n",
    "# Text normalization: handling contractions\n",
    "normalized_sentence = contractions.fix(normalized_sentence)\n",
    "print(\"\\nAfter Handling Contractions:\")\n",
    "print(normalized_sentence)\n",
    "\n",
    "# Tokenization and Lemmatization using NLTK\n",
    "words = word_tokenize(normalized_sentence)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "normalized_sentence_nltk = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "print(\"\\nAfter Tokenization and Lemmatization (NLTK):\")\n",
    "print(normalized_sentence_nltk)\n",
    "\n",
    "# Tokenization and Lemmatization using SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(normalized_sentence)\n",
    "normalized_sentence_spacy = ' '.join([token.lemma_ for token in doc])\n",
    "print(\"\\nAfter Tokenization and Lemmatization (SpaCy):\")\n",
    "print(normalized_sentence_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "those child be play this game those game I play he play\n",
      "Lemmatized Sentence (NLTK):\n",
      "those child be play this game those game I play he play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Sentence to lemmatize\n",
    "# my_sentence = \"Those children are playing. this game, those games, I play he plays\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = normalized_sentence_spacy\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word and join the results\n",
    "lemmatized_sentence_nltk = ''.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "print (words)\n",
    "print(\"Lemmatized Sentence (NLTK):\")\n",
    "print(lemmatized_sentence_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Sentence (SpaCy):\n",
      "those child be play this game those game I play he play\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sentence to lemmatize\n",
    "# my_sentence = \"Those children are playing. this game, those games, I play he plays\"\n",
    "\n",
    "# Process the sentence with SpaCy\n",
    "# nlp model tokenizes the input sentence into individual words and assigns various linguistic annotations to each token, \n",
    "# such as part-of-speech tags, dependency relationships, and lemmas.\n",
    "doc = nlp(normalized_sentence_spacy)\n",
    "\n",
    "# Lemmatize and join the results\n",
    "lemmatized_sentence_spacy = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "print(\"Lemmatized Sentence (SpaCy):\")\n",
    "print(lemmatized_sentence_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the differences between both tools ?\n",
    "\n",
    "## Conclusion\n",
    "There are multiple libraries that allow you to do lemmatization. Each of them have their particularities.\n",
    "There are also other techniques to \"simplify\" words like [Stemming](https://medium.com/swlh/introduction-to-stemming-vs-lemmatization-nlp-8c69eb43ecfe). Feel free explore those that seems relevant to your use-case.\n",
    "\n",
    "![stemming vs lemmatization](https://miro.medium.com/max/2050/1*ES5bt7IoInIq2YioQp2zcQ.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
