{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "To understand a sentence, us as humans, read each word. We analyze the meaning of it and then we connect words together. It's the same for a machine. So the first step to almost any NLP task will be to split sentences in words.\n",
    "\n",
    "![tokens](https://www.kdnuggets.com/wp-content/uploads/text-tokens-tokenization-manning.jpg)\n",
    "\n",
    "It seems simple said like that, but you also have to slice punctuation, compound words (but not all of them),...\n",
    "\n",
    "It's a time consuming task, that's why people invented a \"Tokenization\" function.\n",
    "\n",
    "Let's have a look at how [Spacy](https://spacy.io/) handles that.\n",
    "\n",
    "## Installation\n",
    "\n",
    "You will need to install Spacy, to do that I let you search on [their website](https://spacy.io/).\n",
    "You will also need to download their English model `en_core_web_sm`. To do that you can type:\n",
    "```shell\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "You can see that Spacy provides us with models in a lot of languages. We won't use them now but keep that in mind. It can be useful for later !\n",
    "\n",
    "## Tokenize the text\n",
    "\n",
    "Now that you installed Spacy, let's take a look at their basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Tokenize this document with SpaCy:\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = #TO COMPLETE\n",
    "\n",
    "# Store the tokens in doc\n",
    "\n",
    "doc = #TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, our text is tokenized, now we can see a lot of interesting features. But first of all, let's see what our tokens look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this:\n",
    "```\n",
    "When\n",
    "Sebastian\n",
    "Thrun\n",
    "started\n",
    "working\n",
    "on\n",
    "self\n",
    "-\n",
    "driving\n",
    "cars\n",
    "at\n",
    "Google\n",
    "in\n",
    "2007\n",
    ",\n",
    "few\n",
    "people\n",
    "outside\n",
    "of\n",
    "the\n",
    "company\n",
    "took\n",
    "him\n",
    "seriously\n",
    ".\n",
    "“\n",
    "I\n",
    "can\n",
    "tell\n",
    "you\n",
    "very\n",
    "senior\n",
    "CEOs\n",
    "of\n",
    "major\n",
    "American\n",
    "car\n",
    "companies\n",
    "would\n",
    "shake\n",
    "my\n",
    "hand\n",
    "and\n",
    "turn\n",
    "away\n",
    "because\n",
    "I\n",
    "was\n",
    "n’t\n",
    "worth\n",
    "talking\n",
    "to\n",
    ",\n",
    "”\n",
    "said\n",
    "Thrun\n",
    ",\n",
    "in\n",
    "an\n",
    "interview\n",
    "with\n",
    "Recode\n",
    "earlier\n",
    "this\n",
    "week\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the text to be tokenized\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the tokens\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the punctuation and `-` have been separated from the word they were appended to.\n",
    "\n",
    "Spacy also applies a lot of other preprocessing steps that we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Tokenization ###\n",
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self\n",
      "-\n",
      "driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n",
      "“\n",
      "I\n",
      "can\n",
      "tell\n",
      "you\n",
      "very\n",
      "senior\n",
      "CEOs\n",
      "of\n",
      "major\n",
      "American\n",
      "car\n",
      "companies\n",
      "would\n",
      "shake\n",
      "my\n",
      "hand\n",
      "and\n",
      "turn\n",
      "away\n",
      "because\n",
      "I\n",
      "was\n",
      "n’t\n",
      "worth\n",
      "talking\n",
      "to\n",
      ",\n",
      "”\n",
      "said\n",
      "Thrun\n",
      ",\n",
      "in\n",
      "an\n",
      "interview\n",
      "with\n",
      "Recode\n",
      "earlier\n",
      "this\n",
      "week\n",
      ".\n",
      "\n",
      "### Part-of-Speech Tagging ###\n",
      "When SCONJ\n",
      "Sebastian ADJ\n",
      "Thrun PROPN\n",
      "started VERB\n",
      "working VERB\n",
      "on ADP\n",
      "self NOUN\n",
      "- PUNCT\n",
      "driving VERB\n",
      "cars NOUN\n",
      "at ADP\n",
      "Google PROPN\n",
      "in ADP\n",
      "2007 NUM\n",
      ", PUNCT\n",
      "few ADJ\n",
      "people NOUN\n",
      "outside ADP\n",
      "of ADP\n",
      "the DET\n",
      "company NOUN\n",
      "took VERB\n",
      "him PRON\n",
      "seriously ADV\n",
      ". PUNCT\n",
      "“ PUNCT\n",
      "I PRON\n",
      "can AUX\n",
      "tell VERB\n",
      "you PRON\n",
      "very ADV\n",
      "senior ADJ\n",
      "CEOs NOUN\n",
      "of ADP\n",
      "major ADJ\n",
      "American ADJ\n",
      "car NOUN\n",
      "companies NOUN\n",
      "would AUX\n",
      "shake VERB\n",
      "my PRON\n",
      "hand NOUN\n",
      "and CCONJ\n",
      "turn VERB\n",
      "away ADV\n",
      "because SCONJ\n",
      "I PRON\n",
      "was AUX\n",
      "n’t PART\n",
      "worth ADJ\n",
      "talking VERB\n",
      "to ADP\n",
      ", PUNCT\n",
      "” PUNCT\n",
      "said VERB\n",
      "Thrun PROPN\n",
      ", PUNCT\n",
      "in ADP\n",
      "an DET\n",
      "interview NOUN\n",
      "with ADP\n",
      "Recode PROPN\n",
      "earlier ADV\n",
      "this DET\n",
      "week NOUN\n",
      ". PUNCT\n",
      "\n",
      "### Named Entity Recognition (NER) ###\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n",
      "\n",
      "### Dependency Parsing ###\n",
      "When advmod started VERB\n",
      "Sebastian amod Thrun PROPN\n",
      "Thrun nsubj started VERB\n",
      "started advcl took VERB\n",
      "working xcomp started VERB\n",
      "on prep working VERB\n",
      "self npadvmod driving VERB\n",
      "- punct driving VERB\n",
      "driving amod cars NOUN\n",
      "cars pobj on ADP\n",
      "at prep working VERB\n",
      "Google pobj at ADP\n",
      "in prep working VERB\n",
      "2007 pobj in ADP\n",
      ", punct took VERB\n",
      "few amod people NOUN\n",
      "people nsubj took VERB\n",
      "outside prep people NOUN\n",
      "of prep outside ADP\n",
      "the det company NOUN\n",
      "company pobj of ADP\n",
      "took ROOT took VERB\n",
      "him dobj took VERB\n",
      "seriously advmod took VERB\n",
      ". punct took VERB\n",
      "“ punct tell VERB\n",
      "I nsubj tell VERB\n",
      "can aux tell VERB\n",
      "tell ccomp said VERB\n",
      "you dative tell VERB\n",
      "very advmod senior ADJ\n",
      "senior amod CEOs NOUN\n",
      "CEOs nsubj shake VERB\n",
      "of prep CEOs NOUN\n",
      "major amod companies NOUN\n",
      "American amod companies NOUN\n",
      "car compound companies NOUN\n",
      "companies pobj of ADP\n",
      "would aux shake VERB\n",
      "shake ccomp tell VERB\n",
      "my poss hand NOUN\n",
      "hand dobj shake VERB\n",
      "and cc shake VERB\n",
      "turn conj shake VERB\n",
      "away advmod turn VERB\n",
      "because mark was AUX\n",
      "I nsubj was AUX\n",
      "was advcl turn VERB\n",
      "n’t neg was AUX\n",
      "worth acomp was AUX\n",
      "talking xcomp worth ADJ\n",
      "to prep talking VERB\n",
      ", punct said VERB\n",
      "” punct said VERB\n",
      "said ROOT said VERB\n",
      "Thrun nsubj said VERB\n",
      ", punct Thrun PROPN\n",
      "in prep said VERB\n",
      "an det interview NOUN\n",
      "interview pobj in ADP\n",
      "with prep interview NOUN\n",
      "Recode pobj with ADP\n",
      "earlier advmod week NOUN\n",
      "this det week NOUN\n",
      "week npadvmod interview NOUN\n",
      ". punct said VERB\n",
      "\n",
      "### Lemmatization ###\n",
      "When when\n",
      "Sebastian sebastian\n",
      "Thrun Thrun\n",
      "started start\n",
      "working work\n",
      "on on\n",
      "self self\n",
      "- -\n",
      "driving drive\n",
      "cars car\n",
      "at at\n",
      "Google Google\n",
      "in in\n",
      "2007 2007\n",
      ", ,\n",
      "few few\n",
      "people people\n",
      "outside outside\n",
      "of of\n",
      "the the\n",
      "company company\n",
      "took take\n",
      "him he\n",
      "seriously seriously\n",
      ". .\n",
      "“ \"\n",
      "I I\n",
      "can can\n",
      "tell tell\n",
      "you you\n",
      "very very\n",
      "senior senior\n",
      "CEOs ceo\n",
      "of of\n",
      "major major\n",
      "American american\n",
      "car car\n",
      "companies company\n",
      "would would\n",
      "shake shake\n",
      "my my\n",
      "hand hand\n",
      "and and\n",
      "turn turn\n",
      "away away\n",
      "because because\n",
      "I I\n",
      "was be\n",
      "n’t not\n",
      "worth worth\n",
      "talking talk\n",
      "to to\n",
      ", ,\n",
      "” \"\n",
      "said say\n",
      "Thrun Thrun\n",
      ", ,\n",
      "in in\n",
      "an an\n",
      "interview interview\n",
      "with with\n",
      "Recode Recode\n",
      "earlier early\n",
      "this this\n",
      "week week\n",
      ". .\n",
      "\n",
      "### Sentence Boundary Detection ###\n",
      "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\n",
      "“I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week.\n",
      "\n",
      "### Tokenization with Spaces ###\n",
      "When \n",
      "Sebastian \n",
      "Thrun \n",
      "started \n",
      "working \n",
      "on \n",
      "self\n",
      "-\n",
      "driving \n",
      "cars \n",
      "at \n",
      "Google \n",
      "in \n",
      "2007\n",
      ", \n",
      "few \n",
      "people \n",
      "outside \n",
      "of \n",
      "the \n",
      "company \n",
      "took \n",
      "him \n",
      "seriously\n",
      ". \n",
      "“\n",
      "I \n",
      "can \n",
      "tell \n",
      "you \n",
      "very \n",
      "senior \n",
      "CEOs \n",
      "of \n",
      "major \n",
      "American \n",
      "car \n",
      "companies \n",
      "would \n",
      "shake \n",
      "my \n",
      "hand \n",
      "and \n",
      "turn \n",
      "away \n",
      "because \n",
      "I \n",
      "was\n",
      "n’t \n",
      "worth \n",
      "talking \n",
      "to\n",
      ",\n",
      "” \n",
      "said \n",
      "Thrun\n",
      ", \n",
      "in \n",
      "an \n",
      "interview \n",
      "with \n",
      "Recode \n",
      "earlier \n",
      "this \n",
      "week\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the text to be tokenized\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "### Tokenization ###\n",
    "print(\"### Tokenization ###\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "### Part-of-Speech Tagging ###\n",
    "print(\"\\n### Part-of-Speech Tagging ###\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "### Named Entity Recognition (NER) ###\n",
    "print(\"\\n### Named Entity Recognition (NER) ###\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "### Dependency Parsing ###\n",
    "print(\"\\n### Dependency Parsing ###\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_)\n",
    "\n",
    "### Lemmatization ###\n",
    "print(\"\\n### Lemmatization ###\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "\n",
    "### Sentence Boundary Detection ###\n",
    "print(\"\\n### Sentence Boundary Detection ###\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "### Tokenization with Spaces ###\n",
    "print(\"\\n### Tokenization with Spaces ###\")\n",
    "for token in doc:\n",
    "    print(token.text_with_ws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize into sentences using nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Many had a little lamb.', 'Her fleece was white as snow']\n"
     ]
    }
   ],
   "source": [
    "text = \"Many had a little lamb. Her fleece was white as snow\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sents = sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Many', 'had', 'a', 'little', 'lamb', '.'], ['Her', 'fleece', 'was', 'white', 'as', 'snow']]\n"
     ]
    }
   ],
   "source": [
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
