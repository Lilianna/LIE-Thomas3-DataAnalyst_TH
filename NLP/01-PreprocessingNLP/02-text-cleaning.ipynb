{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "In order to have an accurate result with your NLP model, you need to give all possible information you can to the model. (Only the ones that are useful and well-formatted, of course.)\n",
    "\n",
    "For example, if you have an image before each important word in a text, or some block of text separated by a lot of spaces.\n",
    "\n",
    "Let's take a concrete use-case:\n",
    "\n",
    "![text](https://i.imgur.com/2METpwn.png)\n",
    "\n",
    "In this image, you could provide the text like this:\n",
    "\n",
    "```\n",
    "Becode 1st December 2020 Cantersteen 10 Bruxelles 1000 Bruxelles Dear learners,\n",
    "```\n",
    "\n",
    "But it will be hard for your model to extract meaningful informations out of it. Even for you, if I give you this text it will not be easy.\n",
    "\n",
    "A first solution could be to format and sort it.\n",
    "\n",
    "```\n",
    "Becode\n",
    "Cantersteen 10\n",
    "1000 Bruxelles\n",
    "\n",
    "1st December 2020\n",
    "Bruxelles\n",
    "\n",
    "Dear learners,\n",
    "```\n",
    "\n",
    "A bit better, but it's still not perfect because the model doesn't understand your line breaks, it only understands text and spaces (which are a part of text, too). So we can add a tag. \n",
    "\n",
    "## Html tags\n",
    "\n",
    "![html](https://cdn.lynda.com/course/170427/170427-637363828865101045-16x9.jpg)\n",
    "\n",
    "As a convention, people often use the same tag as the following HTML tag: `<br>` which stands for **B**reak **L**line.\n",
    "\n",
    "So we can do something like:\n",
    "```html\n",
    "Becode\n",
    "Cantersteen 10\n",
    "1000 Bruxelles\n",
    "\n",
    "1st December 2020\n",
    "Bruxelles\n",
    "<br>\n",
    "Dear learners,\n",
    "```\n",
    "\n",
    "As you saw in previous [chapters](../../2.python/2.python_advanced/05.Scraping/2.beautifulsoup_advanced.ipynb), HTML tags will also be important when scraping data from the web.\n",
    "\n",
    "## Create your own tags\n",
    "\n",
    "Sometimes, you want to add visual information that is not in the text. It could be emojis, recurrent images at specific places in front of the text, etc...\n",
    "\n",
    "In those cases, you can create your own tags, but **be careful to only do that if**:\n",
    "1. You are sure that this information will help the model\n",
    "2. There is enough repetition of this tag to allow the model to understand the meaning of it.\n",
    "\n",
    "For example in our letter, we could specify to the model that the address and the date are on a different side of the page. We could. decide to add a tag `<LEFT_SECTION>` (it's just a choice made to call it like that). If I only give this document or add other document that doesn't contain this tag, the model will not understand the meaning of it! But if I give 100 documents like that with the same tag each time, the model could start to understand the link.\n",
    "\n",
    "```\n",
    "Becode\n",
    "Cantersteen 10\n",
    "1000 Bruxelles\n",
    "\n",
    "<LEFT_SECTION>\n",
    "1st December 2020\n",
    "Bruxelles\n",
    "</LEFT_SECTION>\n",
    "\n",
    "\n",
    "Dear learners,\n",
    "```\n",
    "\n",
    "## Tags are sometime dangerous\n",
    "\n",
    "**Pro tips:** Sometimes, you will find those tags in the extracted text, you should always ask yourself: *Does it make sense or is it confusing?*\n",
    "\n",
    "For example, in this text:\n",
    "\n",
    "```\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
    "\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of\n",
    "\n",
    " type and scrambled it to make a type specimen book. \n",
    "```\n",
    "\n",
    "Here the line break doesn't add any information, it's more for the style and the readability.\n",
    "So if you extract those line breaks (and you will with some document formats), you get\n",
    "```\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry.<br>\n",
    "\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of<br>\n",
    "\n",
    " type and scrambled it to make a type specimen book. \n",
    "```\n",
    "\n",
    "You should remove them! You could, for example, use regular expressions for that.\n",
    "\n",
    "You will also encounter formatting tags like `<b>` or `<i>` (bold and italic). Once again, depending on your task, you may want to remove them. If you do document classification it can totally bias the model, if you do Named Entity Recognition (which we'll see in a later chapter), it could definitely help the model.\n",
    "\n",
    "Try to always ask yourself the question: *Would it help me to do the task or not?* If the answer is no, just remove them.\n",
    "\n",
    "## Practice time!\n",
    "\n",
    "Remove all the HTML tags in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<p> WWF's mission is to stop the <strong> degradation </strong> of our planet's natural environment. </p>\"\n",
    "\n",
    "# Remove all HTML tags in 'text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercase the text and spaces\n",
    "\n",
    "## Text casing\n",
    "\n",
    "The text casing will also have an influence on your model. If you are looking for address, names, and so on, it will help the model. But if you want to do document classification, the model will do a difference between `doctor` and `Doctor` and you don't want that. One way to avoid this is by changing all the text to lowercase.\n",
    "\n",
    "## Space trailing\n",
    "\n",
    "When you extract text from documents, sometimes you will have additional spaces after a sentence or a double space where there shouldn't be one. These are just formatting errors, but your model will be affected.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    " This  is   some text where some spaces  have been added.  \n",
    "   Remove them! \n",
    "```\n",
    "\n",
    "In this text it would be easy to remove the spaces but you will not do it by hand for each documents!\n",
    "\n",
    "\n",
    "## Practice time!\n",
    "\n",
    "Remove all the double spaces and the single space at the start or end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" Lorem Ipsum is  simply dummy text  of the printing and typesetting industry. Lorem Ipsum has been  the industry's standard dummy text ever since the  1500s, when an unknown printer took a galley  of type and scrambled it to  make a type specimen  book. It has survived  not only five centuries, but  also the leap  into electronic typesetting ,  remaining essentially unchanged . It was popularised  in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop  publishing software like Aldus PageMaker including versions of Lorem Ipsum. \"\n",
    "\n",
    "\n",
    "# Remove all the useless spaces in 'text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "## What are stop words?\n",
    "\n",
    "A stop word is a word that has very little meaning by itself, such as `the`,`a`, `and`, `an`,...\n",
    "Most search engines remove these \"stop words\" when you do a search.\n",
    "\n",
    "![stop words](https://i2.wp.com/xpo6.com/wp-content/uploads/2009/04/stop-words.png?fit=837%2C499)\n",
    "\n",
    "## How to remove these stop words?\n",
    "\n",
    "You could remove them by hand with the `replace()` function, but if you want to go faster, you can use libraries like `SpaCy`, `NLTK`,  `Gensim`, and more. Each library will behave slightly differently, but not enough to make big changes to your model.\n",
    "\n",
    "## Practice time!\n",
    "\n",
    "Using the library of your choice, remove all the stop words of this text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "becode like learn sometime play games win price fun\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove all my stop words\n",
    "text = \"At BeCode, we like to learn. Sometime, we play games not win a price but to have fun!\"\n",
    "\n",
    "# Remove extra whitespaces\n",
    "cleaned_text = ' '.join(text.split())\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_text = cleaned_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Convert to lowercase\n",
    "cleaned_text = cleaned_text.lower()\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = ' '.join(word for word in cleaned_text.split() if word not in stop_words)\n",
    "\n",
    "print (filtered_text)\n",
    "# You can use any library!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be something like this:\n",
    "```\n",
    "['BeCode', ',', 'like', 'learn', '.', ',', 'play', 'games', 'win', 'price', 'fun', '!']\n",
    "```\n",
    "\n",
    "So as you can see, depending on what kind of information you want to extract, you will be able to exclude stop words. For document classification or semantic search, you will not need those stop words for example.\n",
    "\n",
    "## Customize your stop words\n",
    "\n",
    "You can also add or remove stop words from the list that the libraries uses for stop words. If there is a specific word in your document that should not be considered as a stop word, or one that should absolutely be given to the model, you can do so.\n",
    "\n",
    "\n",
    "## Additional resources\n",
    "* [NLP Essentials: Removing stopwords and performing Text Normalization using NLTK and spaCy in Python](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/)\n",
    "* [Removing stop words from strings in Python](https://stackabuse.com/removing-stop-words-from-strings-in-python/#usingpythonsnltklibrary)\n",
    "* [Dropping common terms: stop words](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All steps for cleaning text: stopwords, html, punctuation, lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Libraries and its uses**: \n",
    "    - BeautifulSoup is a library for pulling data out of HTML and XML files. It's used here to remove HTML tags from the text.\n",
    "        - soup.get_text() is used to extract the text content without HTML tags.\n",
    "        - white space:   ' '.join() joins the words into a string with a single space between each word; after split text into lists of words split() \n",
    "\n",
    "    - re (regular expressions) is used for pattern matching and replacing.\n",
    "    - string provides a collection of common string operations.\n",
    "        - cleaned_text.translate(str.maketrans(\"\", \"\", string.punctuation)) removes punctuation using a translation table.\n",
    "    - stopwords from NLTK (Natural Language Toolkit) is used for a predefined list of stop words.\n",
    "        - stopwords.words('english') provides a list of English stop words.\n",
    "        - filtered_text is created by excluding stop words from the cleaned text.\n",
    "    - nltk.download('stopwords') is used to download the NLTK stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text without HTML tags:\n",
      "wwfs mission is to stop the degradation of our planets natural environment\n",
      "\n",
      "Cleaned Text without HTML tags, extra spaces, and stop words:\n",
      "wwfs mission stop degradation planets natural environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\becode\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Sample text with HTML tags\n",
    "text_with_html = \"<p> WWF's mission is to stop the <strong> degradation </strong> of our planet's natural environment. </p>\"\n",
    "\n",
    "# Remove HTML tags\n",
    "soup = BeautifulSoup(text_with_html, 'html.parser')\n",
    "cleaned_text = soup.get_text()\n",
    "\n",
    "# Remove extra whitespaces\n",
    "cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_text = cleaned_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Convert to lowercase\n",
    "cleaned_text = cleaned_text.lower()\n",
    "\n",
    "# Sample text with stop words\n",
    "text_with_stopwords = \"At BeCode, we like to learn. Sometime, we play games not win a price but to have fun!\"\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = ' '.join(word for word in cleaned_text.split() if word not in stop_words)\n",
    "\n",
    "print(\"Cleaned Text without HTML tags:\")\n",
    "print(cleaned_text)\n",
    "print(\"\\nCleaned Text without HTML tags, extra spaces, and stop words:\")\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalisation\n",
    "- text normalization, including lowercasing, removing punctuation, handling contractions, removing special characters, and tokenization with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "At BeCode, we like to learn. Sometimes, we play games not to win a prize but to have fun! Join us at 3 PM for the event.\n",
      "\n",
      "After Lowercasing:\n",
      "at becode, we like to learn. sometimes, we play games not to win a prize but to have fun! join us at 3 pm for the event.\n",
      "\n",
      "After Removing Punctuation:\n",
      "at becode we like to learn sometimes we play games not to win a prize but to have fun join us at 3 pm for the event\n",
      "\n",
      "After Handling Contractions:\n",
      "at becode we like to learn sometimes we play games not to win a prize but to have fun join us at 3 pm for the event\n",
      "\n",
      "After Tokenization and Lemmatization:\n",
      "at becode we like to learn sometimes we play game not to win a prize but to have fun join we at 3 pm for the event\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import contractions   \n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Install the contractions library if not already installed\n",
    "# !pip install contractions\n",
    "\n",
    "# Example text with variations, including numbers\n",
    "example_text = \"At BeCode, we like to learn. Sometimes, we play games not to win a prize but to have fun! Join us at 3 PM for the event.\"\n",
    "print(\"Original Text:\")\n",
    "print(example_text)\n",
    "\n",
    "# Lowercasing\n",
    "normalized_text = example_text.lower()\n",
    "print(\"\\nAfter Lowercasing:\")\n",
    "print(normalized_text)\n",
    "\n",
    "# Removing Punctuation\n",
    "normalized_text = normalized_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(\"\\nAfter Removing Punctuation:\")\n",
    "print(normalized_text)\n",
    "\n",
    "# Handling Contractions\n",
    "normalized_text = contractions.fix(normalized_text)\n",
    "print(\"\\nAfter Handling Contractions:\")\n",
    "print(normalized_text)\n",
    "\n",
    "# Tokenization and Lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(normalized_text)\n",
    "normalized_text = ' '.join([token.lemma_ for token in doc])\n",
    "print(\"\\nAfter Tokenization and Lemmatization:\")\n",
    "print(normalized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
