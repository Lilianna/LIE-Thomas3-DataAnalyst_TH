{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_words))\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from %pip install tensorflow\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "# Load IMDB dataset (as an example)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# Placeholder values for demonstration\n",
    "vocab_size = 10000\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=250, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "\n",
    "# Step 1: Download IMDb dataset\n",
    "def download_imdb_dataset():\n",
    "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    file_name = \"aclImdb_v1.tar.gz\"\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "    with tarfile.open(file_name, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall()\n",
    "    os.remove(file_name)\n",
    "    \n",
    "# Download IMDb dataset\n",
    "download_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_data(max_words, max_len):\n",
    "    # Load IMDb dataset\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "    # Print insights about the dataset\n",
    "    print(\"Number of unique words:\", len(set([word for sequence in X_train + X_test for word in sequence])))\n",
    "    print(\"Average length of reviews:\")\n",
    "    for seq in [X_train, X_test]:\n",
    "        print(sum(len(s) for s in seq) / len(seq))\n",
    "\n",
    "    # Pad sequences\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 250)               4000250   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4320501 (16.48 MB)\n",
      "Trainable params: 4320501 (16.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "196/196 - 50s - loss: 0.4694 - accuracy: 0.7558 - val_loss: 0.3012 - val_accuracy: 0.8728 - 50s/epoch - 255ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 54s - loss: 0.1568 - accuracy: 0.9423 - val_loss: 0.3168 - val_accuracy: 0.8708 - 54s/epoch - 275ms/step\n",
      "Accuracy: 87.08%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "# Modeling\n",
    "def build_model(max_words, max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_len))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=250, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the model\n",
    "model = build_model(max_words, max_len)\n",
    "model.summary()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
